{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPACA_API_KEY = \"PKVQE7BUOAC40D5OSCHH\"\n",
    "ALPACA_SECRET_KEY = \"DfXHYuZpMlmON3mcHPVTQDWZTnnMyt95PyaDtlAF\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">ENPH</th>\n",
       "      <th colspan=\"5\" halign=\"left\">SPCE</th>\n",
       "      <th colspan=\"5\" halign=\"left\">SQ</th>\n",
       "      <th colspan=\"5\" halign=\"left\">UBER</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:00:00-04:00</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.2000</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>12.975</td>\n",
       "      <td>15294259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.95</td>\n",
       "      <td>74.73</td>\n",
       "      <td>73.171</td>\n",
       "      <td>73.67</td>\n",
       "      <td>12270720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-02 00:00:00-04:00</th>\n",
       "      <td>12.40</td>\n",
       "      <td>13.0400</td>\n",
       "      <td>12.3600</td>\n",
       "      <td>12.940</td>\n",
       "      <td>3509876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.80</td>\n",
       "      <td>70.50</td>\n",
       "      <td>66.050</td>\n",
       "      <td>67.73</td>\n",
       "      <td>33369047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-03 00:00:00-04:00</th>\n",
       "      <td>12.93</td>\n",
       "      <td>13.8792</td>\n",
       "      <td>12.9300</td>\n",
       "      <td>13.830</td>\n",
       "      <td>3954682</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.91</td>\n",
       "      <td>68.67</td>\n",
       "      <td>66.600</td>\n",
       "      <td>68.52</td>\n",
       "      <td>8847087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-06 00:00:00-04:00</th>\n",
       "      <td>13.50</td>\n",
       "      <td>13.8950</td>\n",
       "      <td>13.4301</td>\n",
       "      <td>13.740</td>\n",
       "      <td>2394985</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.46</td>\n",
       "      <td>70.29</td>\n",
       "      <td>65.410</td>\n",
       "      <td>70.21</td>\n",
       "      <td>12633902</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-07 00:00:00-04:00</th>\n",
       "      <td>13.65</td>\n",
       "      <td>14.2900</td>\n",
       "      <td>13.6500</td>\n",
       "      <td>14.210</td>\n",
       "      <td>3189606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.98</td>\n",
       "      <td>70.34</td>\n",
       "      <td>67.370</td>\n",
       "      <td>68.41</td>\n",
       "      <td>10006668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ENPH                                     SPCE  \\\n",
       "                            open     high      low   close    volume open   \n",
       "time                                                                        \n",
       "2019-05-01 00:00:00-04:00  12.30  13.2000  12.0000  12.975  15294259  NaN   \n",
       "2019-05-02 00:00:00-04:00  12.40  13.0400  12.3600  12.940   3509876  NaN   \n",
       "2019-05-03 00:00:00-04:00  12.93  13.8792  12.9300  13.830   3954682  NaN   \n",
       "2019-05-06 00:00:00-04:00  13.50  13.8950  13.4301  13.740   2394985  NaN   \n",
       "2019-05-07 00:00:00-04:00  13.65  14.2900  13.6500  14.210   3189606  NaN   \n",
       "\n",
       "                                                    SQ                        \\\n",
       "                          high low close volume   open   high     low  close   \n",
       "time                                                                           \n",
       "2019-05-01 00:00:00-04:00  NaN NaN   NaN    NaN  73.95  74.73  73.171  73.67   \n",
       "2019-05-02 00:00:00-04:00  NaN NaN   NaN    NaN  69.80  70.50  66.050  67.73   \n",
       "2019-05-03 00:00:00-04:00  NaN NaN   NaN    NaN  67.91  68.67  66.600  68.52   \n",
       "2019-05-06 00:00:00-04:00  NaN NaN   NaN    NaN  65.46  70.29  65.410  70.21   \n",
       "2019-05-07 00:00:00-04:00  NaN NaN   NaN    NaN  69.98  70.34  67.370  68.41   \n",
       "\n",
       "                                    UBER                        \n",
       "                             volume open high low close volume  \n",
       "time                                                            \n",
       "2019-05-01 00:00:00-04:00  12270720  NaN  NaN NaN   NaN    NaN  \n",
       "2019-05-02 00:00:00-04:00  33369047  NaN  NaN NaN   NaN    NaN  \n",
       "2019-05-03 00:00:00-04:00   8847087  NaN  NaN NaN   NaN    NaN  \n",
       "2019-05-06 00:00:00-04:00  12633902  NaN  NaN NaN   NaN    NaN  \n",
       "2019-05-07 00:00:00-04:00  10006668  NaN  NaN NaN   NaN    NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set Alpaca API key and secret\n",
    "alpaca_api_key = ALPACA_API_KEY\n",
    "alpaca_secret_key = ALPACA_SECRET_KEY\n",
    " \n",
    "api = tradeapi.REST(\n",
    "   alpaca_api_key,\n",
    "   alpaca_secret_key,\n",
    "   api_version = \"v2\"\n",
    ")\n",
    "# Set timeframe to '1D'\n",
    "timeframe = \"1D\"\n",
    "# Set start and end datetimes between now and 3 years ago.\n",
    "start_date = pd.Timestamp(\"2019-05-01\", tz=\"America/New_York\").isoformat()\n",
    "end_date = pd.Timestamp(\"2021-06-27\", tz=\"America/New_York\").isoformat()\n",
    "# Set the ticker information\n",
    "tickers = [\"SQ\",\"ENPH\",\"UBER\",\"SPCE\"]\n",
    "# Get 3 year's worth of historical price data for Microsoft and Coca-Cola\n",
    "df_ticker = api.get_barset(\n",
    "   tickers,\n",
    "   timeframe,\n",
    "   start=start_date,\n",
    "   end=end_date,\n",
    "   limit=1000,\n",
    ").df\n",
    "# Display sample data\n",
    "df_ticker.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of time\n",
       "2019-05-01 00:00:00-04:00     73.67\n",
       "2019-05-02 00:00:00-04:00     67.73\n",
       "2019-05-03 00:00:00-04:00     68.52\n",
       "2019-05-06 00:00:00-04:00     70.21\n",
       "2019-05-07 00:00:00-04:00     68.41\n",
       "                              ...  \n",
       "2021-06-21 00:00:00-04:00    233.91\n",
       "2021-06-22 00:00:00-04:00    235.98\n",
       "2021-06-23 00:00:00-04:00    238.69\n",
       "2021-06-24 00:00:00-04:00    244.28\n",
       "2021-06-25 00:00:00-04:00    240.24\n",
       "Name: close, Length: 544, dtype: float64>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQ_df = df_ticker[\"SQ\"].close\n",
    "SQ_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 73.67    67.73    68.52    70.21    68.41    67.28    66.38    65.99\n",
      "  62.38    64.12    65.2     66.41    65.3     64.38    65.94    66.4472\n",
      "  63.75    64.66    65.84    63.23    63.65    61.95    60.63    63.3997\n",
      "  64.93    65.79    68.45    70.37    70.17    70.49    72.18    71.58\n",
      "  72.24    71.8346  72.65    74.42    72.9     72.705   69.86    69.96\n",
      "  71.3     72.505   73.22    73.97    74.2638  74.32    73.41    78.03\n",
      "  78.38    79.445   82.33    82.1019  80.57    80.86    81.03    78.48\n",
      "  78.57    78.85    80.42    80.04    81.81    80.23    80.48    80.4\n",
      "  81.1435  69.599   64.8475  64.6     65.01    66.3     65.      62.29\n",
      "  63.01    61.53    61.99    62.87    64.11    64.4     65.42    64.67\n",
      "  61.83    62.82    61.74    62.14    62.85    61.81    61.15    61.09\n",
      "  63.045   61.98    61.45    60.5     59.18    57.79    58.32    59.25\n",
      "  59.54    59.73    59.03    57.83    56.75    57.53    58.51    60.8\n",
      "  60.76    62.005   61.61    60.06    62.06    62.64    62.25    60.81\n",
      "  62.235   62.01    61.53    61.83    64.29    63.01    63.38    60.46\n",
      "  61.15    58.82    58.34    61.69    62.8701  62.82    62.41    62.1859\n",
      "  61.46    62.61    62.39    61.05    61.34    64.41    62.49    63.77\n",
      "  61.53    61.54    62.99    64.7     65.74    66.4     67.18    67.4\n",
      "  67.75    68.21    68.97    69.65    69.16    66.9     67.05    67.82\n",
      "  67.18    67.98    67.11    65.83    65.67    65.85    64.79    66.47\n",
      "  65.55    65.13    64.5     63.61    62.8     63.96    64.05    63.82\n",
      "  61.8     62.56    63.83    63.      62.57    64.57    67.6     67.92\n",
      "  67.99    68.66    68.16    70.37    69.53    69.38    68.31    68.69\n",
      "  69.29    71.16    71.06    74.82    76.16    76.1     74.68    79.795\n",
      "  80.9     78.39    78.5     78.85    80.17    79.88    79.98    81.\n",
      "  81.86    82.425   85.24    85.74    83.49    80.22    77.03    76.66\n",
      "  79.32    83.34    80.7     78.82    79.217   76.3221  73.08    65.83\n",
      "  67.25    62.43    53.58    57.78    41.21    44.73    39.4627  39.99\n",
      "  38.03    40.01    46.36    52.44    56.07    53.34    55.      52.39\n",
      "  46.81    45.86    43.75    50.42    50.3     57.06    59.21    59.44\n",
      "  62.41    61.3     57.29    61.08    61.05    57.38    61.1     61.8275\n",
      "  61.975   63.64    61.74    65.75    65.11    62.97    63.68    66.685\n",
      "  68.23    74.63    76.06    74.93    75.2     73.77    78.1927  80.39\n",
      "  76.62    79.23    82.03    81.52    81.5     80.8     80.96    79.65\n",
      "  81.13    82.72    88.0101  91.96    87.72    90.05    89.59    89.6\n",
      "  91.96    86.12    85.76    91.09    94.79    97.01    98.27    97.99\n",
      " 104.24   104.74   100.88   104.88   104.3    103.67   104.85   115.9\n",
      " 113.46   118.975  125.62   133.39   129.39   128.07   118.74   121.24\n",
      " 122.2    119.96   120.72   128.39   127.     129.12   122.94   121.41\n",
      " 125.74   123.51   128.53   129.06   129.85   134.83   136.8    146.38\n",
      " 153.98   147.225  139.26   138.255  139.02   143.16   141.91   152.47\n",
      " 150.96   150.37   156.08   155.12   151.835  155.     153.8    155.62\n",
      " 155.85   159.47   166.67   162.875  152.87   146.4    139.11   144.22\n",
      " 141.01   137.45   144.3    154.49   151.14   145.674  145.08   151.03\n",
      " 155.52   149.72   152.36   157.79   160.7    164.81   162.56   168.6\n",
      " 169.61   181.     175.31   180.15   183.5    187.25   185.25   190.56\n",
      " 187.49   188.61   186.35   186.95   185.68   177.92   175.93   176.77\n",
      " 169.89   171.01   167.13   169.86   154.87   155.23   158.22   171.29\n",
      " 175.56   198.0993 184.     172.09   183.38   178.5    177.16   179.09\n",
      " 187.84   185.53   191.58   195.97   207.69   202.955  212.93   212.5\n",
      " 210.98   203.     202.11   205.53   208.14   212.59   213.01   207.\n",
      " 217.25   216.6    215.88   220.     226.87   230.75   235.525  233.39\n",
      " 241.57   230.72   228.28   223.35   214.     221.03   217.71   221.1\n",
      " 229.75   227.02   239.59   241.48   225.5    227.47   230.94   232.71\n",
      " 227.65   227.25   226.65   219.62   222.88   216.63   209.49   202.3\n",
      " 219.91   215.92   221.94   227.63   227.76   237.795  240.38   259.82\n",
      " 258.13   257.47   265.87   272.7501 276.02   271.09   270.85   276.57\n",
      " 268.28   256.62   237.31   226.78   230.2    241.     252.15   233.96\n",
      " 218.39   216.4399 201.8688 225.01   226.8    241.67   242.1    251.17\n",
      " 243.36   246.41   224.27   224.84   226.12   223.08   213.53   208.47\n",
      " 213.61   207.1    212.79   226.935  229.51   229.8601 236.58   245.\n",
      " 258.32   261.65   265.2    273.1182 258.34   263.145  256.11   245.14\n",
      " 245.575  245.43   245.2    246.38   255.71   253.91   254.22   247.76\n",
      " 245.     243.52   231.18   231.76   224.03   233.05   216.43   220.66\n",
      " 206.55   197.185  207.85   202.91   203.15   200.15   204.9    200.025\n",
      " 210.87   216.04   222.07   220.9    222.61   221.91   220.475  211.4\n",
      " 213.69   216.94   214.1535 210.21   217.12   219.34   230.95   227.69\n",
      " 225.15   236.25   237.17   233.91   235.98   238.69   244.28   240.24  ]\n"
     ]
    }
   ],
   "source": [
    "#SQ_X = SQ_df.to_numpy()\r\n",
    "#print (SQ_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before the for loop, declare variables for each ticker for each model     i.e.  square linear regression/ square RNN\n",
    "\n",
    "# Create For Loop; \n",
    "# Iterate through each ticker\n",
    "# get closing value from alpacas data frame from each ticker\n",
    "\n",
    "# Convert to numpy\n",
    "# Run SKlearn code\n",
    "# Run RNN code\n",
    "\n",
    "# save output of the models to these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.29300000e+01 7.31699980e+01 6.92600020e+01 6.98700030e+01\n",
      "  6.98700030e+01 1.20327000e+07]\n",
      " [7.09800030e+01 7.20000000e+01 6.99100040e+01 6.99499970e+01\n",
      "  6.99499970e+01 7.91180000e+06]\n",
      " [7.05700000e+01 7.15400010e+01 7.00500030e+01 7.12799990e+01\n",
      "  7.12799990e+01 5.16380000e+06]\n",
      " ...\n",
      " [2.34910004e+02 2.35660004e+02 2.28820007e+02 2.33889999e+02\n",
      "  2.33889999e+02 6.62890000e+06]\n",
      " [2.32889999e+02 2.36278000e+02 2.29807007e+02 2.35970001e+02\n",
      "  2.35970001e+02 6.74890000e+06]\n",
      " [2.36899994e+02 2.43000000e+02 2.36860001e+02 2.38699997e+02\n",
      "  2.38699997e+02 8.16550000e+06]]\n"
     ]
    }
   ],
   "source": [
    "X = df.to_numpy()\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "price = df[[\"Close\"]]\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "Y = scaler.fit_transform(price[\"Close\"].values.reshape(-1,1))\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.71120888 -0.73112431 -0.69385724 -0.73347868 -0.73347868 -0.53668933]\n",
      " [-0.72755078 -0.74076821 -0.68846885 -0.73280782 -0.73280782 -0.73652155]\n",
      " [-0.7309868  -0.74455983 -0.68730828 -0.72165382 -0.72165382 -0.86977858]\n",
      " ...\n",
      " [ 0.64626028  0.6082262   0.62886521  0.64206636  0.64206636 -0.79873241]\n",
      " [ 0.62933168  0.61332013  0.63704725  0.65951019  0.65951019 -0.79291332]\n",
      " [ 0.66293734  0.66872732  0.69551525  0.68240516  0.68240516 -0.72421903]]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "#print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, lookback, split_ratio):\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for index in range(len(X) - lookback): \n",
    "        data.append(X[index: index + lookback])\n",
    "\n",
    "    data = np.array(data);\n",
    "    test_set_size = int(np.round(split_ratio * data.shape[0]))\n",
    "    train_set_size = data.shape[0] - (test_set_size)\n",
    "    \n",
    "    x_train = data[:train_set_size,:]\n",
    "    y_train = Y[:train_set_size,:]\n",
    "    \n",
    "    x_test = data[train_set_size:,:]\n",
    "    y_test = Y[train_set_size:,:]\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "lookback = 40 # sequence length\n",
    "split_ratio = 0.1\n",
    "x_train, y_train, x_test, y_test = split_data(X, Y, lookback, split_ratio)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "input_dim = 6\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "num_epochs = 100\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(input_dim = input_dim, hidden_dim = hidden_dim, num_layers = num_layers, output_dim = output_dim)\n",
    "loss_func = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "n_epochs = 100\n",
    "start_time=time.time()\n",
    "for e in range(n_epochs):\n",
    "  y_train_pred = model(x_train)\n",
    "  loss = loss_func(y_train_pred, y_train)\n",
    "  print(f\"Epoch: {e}\\tLoss: {loss.item()}\")\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total time for training: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do you use this on current data in order to get a predicted number?\n",
    "\n",
    "# feed current data into this model?\n",
    " # create for loops\n",
    "# are we validating model?\n",
    "\n",
    "# make predictions and validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3d3bff687f703ad3928cf3e76d3a900c4f06b8bfc69a25eb31bc97c63177b9f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}