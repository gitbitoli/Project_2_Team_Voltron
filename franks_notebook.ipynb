{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from tensorflow.keras.optimizers import RMSprop,SGD\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 134
    }
   ],
   "source": [
    "# Load .env environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = \"PKNHKA7JSFYR088WXJKA\"\n",
    "alpaca_secret_key = \"YyABDvtTosEMl53YfKDoDTZJIK1QP4lpuXw4fFM0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Alpaca Key type: <class 'str'>\nAlpaca Secret Key type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Verify that Alpaca key and secret were correctly loaded\n",
    "print(f\"Alpaca Key type: {type(alpaca_api_key)}\")\n",
    "print(f\"Alpaca Secret Key type: {type(alpaca_secret_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Alpaca API object\n",
    "api = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version = \"v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timeframe to '1D' pulling non-vol stock data\n",
    "timeframe = \"1D\"\n",
    "\n",
    "# Set start and end datetimes between now and 3 years ago.\n",
    "start_date = pd.Timestamp(\"2020-01-01\", tz=\"America/New_York\").isoformat()\n",
    "end_date = pd.Timestamp(\"2021-06-01\", tz=\"America/New_York\").isoformat()\n",
    "# Set the ticker info\n",
    "tickers = [\"SQ\",\"IBM\",\"ENPH\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                            ENPH                                    IBM  \\\n",
       "                            open   high    low   close   volume    open   \n",
       "time                                                                      \n",
       "2020-01-02 00:00:00-05:00  26.37  29.35  26.04  29.340  7582721  135.00   \n",
       "2020-01-03 00:00:00-05:00  28.79  29.73  28.38  29.280  4682579  133.57   \n",
       "2020-01-06 00:00:00-05:00  29.05  29.66  27.83  29.650  4462305  133.42   \n",
       "2020-01-07 00:00:00-05:00  29.82  30.20  28.66  29.995  5082191  133.69   \n",
       "2020-01-08 00:00:00-05:00  29.68  31.50  29.44  31.000  5240083  134.51   \n",
       "\n",
       "                                                                 SQ         \\\n",
       "                             high       low   close   volume   open   high   \n",
       "time                                                                         \n",
       "2020-01-02 00:00:00-05:00  135.92  134.7701  135.46  2650485  62.99  64.05   \n",
       "2020-01-03 00:00:00-05:00  134.86  133.5600  134.32  1870413  62.59  63.27   \n",
       "2020-01-06 00:00:00-05:00  134.24  133.2000  134.08  2031658  61.36  62.58   \n",
       "2020-01-07 00:00:00-05:00  134.96  133.5633  134.15  2631709  64.57  65.49   \n",
       "2020-01-08 00:00:00-05:00  135.86  133.9200  135.30  3662956  64.56  68.00   \n",
       "\n",
       "                                                   \n",
       "                             low  close    volume  \n",
       "time                                               \n",
       "2020-01-02 00:00:00-05:00  62.95  63.83   4571798  \n",
       "2020-01-03 00:00:00-05:00  62.33  63.00   4870465  \n",
       "2020-01-06 00:00:00-05:00  61.13  62.57   5681852  \n",
       "2020-01-07 00:00:00-05:00  63.66  64.57   8302437  \n",
       "2020-01-08 00:00:00-05:00  64.15  67.60  11170180  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">ENPH</th>\n      <th colspan=\"5\" halign=\"left\">IBM</th>\n      <th colspan=\"5\" halign=\"left\">SQ</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n    </tr>\n    <tr>\n      <th>time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2020-01-02 00:00:00-05:00</th>\n      <td>26.37</td>\n      <td>29.35</td>\n      <td>26.04</td>\n      <td>29.340</td>\n      <td>7582721</td>\n      <td>135.00</td>\n      <td>135.92</td>\n      <td>134.7701</td>\n      <td>135.46</td>\n      <td>2650485</td>\n      <td>62.99</td>\n      <td>64.05</td>\n      <td>62.95</td>\n      <td>63.83</td>\n      <td>4571798</td>\n    </tr>\n    <tr>\n      <th>2020-01-03 00:00:00-05:00</th>\n      <td>28.79</td>\n      <td>29.73</td>\n      <td>28.38</td>\n      <td>29.280</td>\n      <td>4682579</td>\n      <td>133.57</td>\n      <td>134.86</td>\n      <td>133.5600</td>\n      <td>134.32</td>\n      <td>1870413</td>\n      <td>62.59</td>\n      <td>63.27</td>\n      <td>62.33</td>\n      <td>63.00</td>\n      <td>4870465</td>\n    </tr>\n    <tr>\n      <th>2020-01-06 00:00:00-05:00</th>\n      <td>29.05</td>\n      <td>29.66</td>\n      <td>27.83</td>\n      <td>29.650</td>\n      <td>4462305</td>\n      <td>133.42</td>\n      <td>134.24</td>\n      <td>133.2000</td>\n      <td>134.08</td>\n      <td>2031658</td>\n      <td>61.36</td>\n      <td>62.58</td>\n      <td>61.13</td>\n      <td>62.57</td>\n      <td>5681852</td>\n    </tr>\n    <tr>\n      <th>2020-01-07 00:00:00-05:00</th>\n      <td>29.82</td>\n      <td>30.20</td>\n      <td>28.66</td>\n      <td>29.995</td>\n      <td>5082191</td>\n      <td>133.69</td>\n      <td>134.96</td>\n      <td>133.5633</td>\n      <td>134.15</td>\n      <td>2631709</td>\n      <td>64.57</td>\n      <td>65.49</td>\n      <td>63.66</td>\n      <td>64.57</td>\n      <td>8302437</td>\n    </tr>\n    <tr>\n      <th>2020-01-08 00:00:00-05:00</th>\n      <td>29.68</td>\n      <td>31.50</td>\n      <td>29.44</td>\n      <td>31.000</td>\n      <td>5240083</td>\n      <td>134.51</td>\n      <td>135.86</td>\n      <td>133.9200</td>\n      <td>135.30</td>\n      <td>3662956</td>\n      <td>64.56</td>\n      <td>68.00</td>\n      <td>64.15</td>\n      <td>67.60</td>\n      <td>11170180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "small_cap_tick = api.get_barset(\n",
    "   tickers,\n",
    "   timeframe,\n",
    "   start=start_date,\n",
    "   end=end_date,\n",
    "   limit=1000\n",
    ").df\n",
    "# Display sample data\n",
    "small_cap_tick.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {\"Small cap\":[\"WKHS\",\"BRP\",\"DX\"], \"Mid cap\":[\"RIOT\",\"TTD\",\"DXC\"],\"Large cap\":[\"SQ\",\"IBM\",\"ENPH\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ENPH  open      0\n",
       "      high      0\n",
       "      low       0\n",
       "      close     0\n",
       "      volume    0\n",
       "IBM   open      0\n",
       "      high      0\n",
       "      low       0\n",
       "      close     0\n",
       "      volume    0\n",
       "SQ    open      0\n",
       "      high      0\n",
       "      low       0\n",
       "      close     0\n",
       "      volume    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "small_cap_tick.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(Small_cap_tick, window, feature_col_number, target_col_number):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(Small_cap_tick) - window - 1):\n",
    "        features = Small_cap_tick.iloc[i:(i + window), feature_col_number]\n",
    "        target = Small_cap_tick.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function to pull ticker names\n",
    "def training_model(tickers):\n",
    "    Small_cap_tick = api.get_barset(\n",
    "       tickers,\n",
    "       timeframe,\n",
    "       start=start_date,\n",
    "       end=end_date,\n",
    "       limit=1000\n",
    "    ).df\n",
    "    # Display sample data\n",
    "    Small_cap_tick.head()\n",
    "    \n",
    "    #\n",
    "    data = np.array(Small_cap_tick)\n",
    "\n",
    "    # train test split, we can take last 100 data points as test set\n",
    "    train , test = data[0:-100], data[-100:]\n",
    "    \n",
    "   \n",
    "        # Predict Closing Prices using a 10 day window of previous closing prices\n",
    "    # Then, experiment with window sizes anywhere from 1 to 10 and see how the model performance changes\n",
    "    window_size = 10\n",
    "\n",
    "    # Column index 0 is the 'IBM ' column\n",
    "    # Column index 1 is the `Close` column\n",
    "    feature_column = 1\n",
    "    target_column = 1\n",
    "    X, y = window_data(Small_cap_tick, window_size, feature_column, target_column)\n",
    "    wtf = X, y\n",
    "\n",
    "        # Use 70% of the data for training and the remaineder for testing\n",
    "    data_train = int(.7 * len(X))\n",
    "    X_train = X[:data_train - 1]\n",
    "    X_test = X[data_train:]\n",
    "    y_train = y[:data_train - 1]\n",
    "    y_test = y[data_train:]\n",
    "    \n",
    "        # Use the MinMaxScaler to scale data between 0 and 1.\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    x_train_scaler = MinMaxScaler()\n",
    "    x_test_scaler = MinMaxScaler()\n",
    "    y_train_scaler = MinMaxScaler()\n",
    "    y_test_scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler for the training Data\n",
    "    x_train_scaler.fit(X_train)\n",
    "    y_train_scaler.fit(y_train)\n",
    "\n",
    "    # Scale the training data\n",
    "    X_train = x_train_scaler.transform(X_train)\n",
    "    y_train = y_train_scaler.transform(y_train)\n",
    "\n",
    "    # Fit the scaler for the testing Data\n",
    "    x_test_scaler.fit(X_test)\n",
    "    y_test_scaler.fit(y_test)\n",
    "\n",
    "    # Scale the y_test data\n",
    "    X_test = x_test_scaler.transform(X_test)\n",
    "    y_test = y_test_scaler.transform(y_test)\n",
    "\n",
    "        # Reshape the features for the model\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Build the LSTM model. \n",
    "       \n",
    "    # Note: Batching inputs has a different input shape of Samples/TimeSteps/Features\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        units=30, return_sequences=True,\n",
    "        input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=30, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=30))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    opt = SGD(lr=0.01, momentum=0.9, clipnorm=1.0)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "    \n",
    "        # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        # Summarize the model\n",
    "    #model.summary()\n",
    "    \n",
    "        # Train the model\n",
    "    # Use at least 10 epochs\n",
    "    model.fit(X_train, y_train, epochs=50, shuffle=False, batch_size=1,\n",
    "        verbose=1)\n",
    "    \n",
    "        # Evaluate the model\n",
    "    model.evaluate(X_test, y_test)\n",
    "    print (\"test loss\")\n",
    "    print (model.evaluate(X_test, y_test))\n",
    "    \n",
    "            # Make some predictions\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "        # Recover the original prices instead of the scaled version\n",
    "    predicted_prices = y_test_scaler.inverse_transform(predicted)\n",
    "    real_prices = y_test_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "        # Create a DataFrame of Real and Predicted values\n",
    "    stocks = pd.DataFrame({\n",
    "        \"Real\": real_prices.ravel(),\n",
    "        \"Predicted\": predicted_prices.ravel()\n",
    "    }, index = Small_cap_tick.index[-len(real_prices): ]) \n",
    "   \n",
    "    \n",
    "    return stocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_model(Small_cap_tick)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clipnorm=0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-d2743ebf235a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpredicted_prices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mreal_prices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for(name,l) in groups.items():\n",
    "\n",
    "\n",
    "    print (name)\n",
    "    stocks = (training_model(l))\n",
    "    plt.plot(stocks.index, stocks[[\"Real\"]],label=\"Real\")\n",
    "    plt.plot(stocks.index, stocks[[\"Predicted\"]], label=\"Predicted\")\n",
    "    plt.suptitle(name)\n",
    "    plt.xlabel(\"datetime\")\n",
    "    plt.ylabel(\"Normalized closed price\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    #print (training_model(l))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "54bd7b183a5689457db50215dcf96c48b2aeef51ee043d07751f799ca7598ef2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
